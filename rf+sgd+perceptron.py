# -*- coding: utf-8 -*-
"""RF+SGD+Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YoZOuo5_r9igSdtVR-sV1NKsxPQP4MXu
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv("/content/drive/MyDrive/corona.csv")

df.info()

data=[df]
genders={'male':0,'female':1,'None':2}
result={'positive':0,"negative":1,"other":2}
age={'Yes':0,'No':1,'None':2}
for dataset in data:
  dataset['gender']=dataset['gender'].map(genders)
  dataset['corona_result']=dataset['corona_result'].map(result)
  dataset['age_60_and_above']=dataset['age_60_and_above'].map(age)

df=df.drop(['test_date', 'test_indication'], axis=1)

data=[df]
for dataset in data:
  dataset['fever']=dataset['fever'].replace("None",2)
  dataset['cough']=dataset['cough'].replace("None",2)
  dataset['shortness_of_breath']=dataset['shortness_of_breath'].replace("None",2)
  dataset['sore_throat']=dataset['sore_throat'].replace("None",2)
  dataset['head_ache']=dataset['head_ache'].replace("None",2)
  dataset['age_60_and_above']=dataset['age_60_and_above'].replace("None",2)
 
df['fever']=df["fever"].astype(int)
df['age_60_and_above']=df["age_60_and_above"].astype(int)
df['head_ache']=df["head_ache"].astype(int)
df['sore_throat']=df["sore_throat"].astype(int)
df['shortness_of_breath']=df["shortness_of_breath"].astype(int)
df['cough']=df["cough"].astype(int)

df

"""**DATA SPLITTING**"""

#Split data into 80% training & 20% testing data sets
X = df.iloc[:, [0, 1, 2, 3, 4, 6, 7]].values
y = df.iloc[:, [5]].values.ravel()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Creating scaled set to be used in model to improve the results
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""**NAIVE BAYES**"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
y_pred

#Evaluation metrics after prediction of data
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
print(classification_report(y_test ,y_pred ))
print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred))
print()
print('Accuracy: ', accuracy_score(y_test,y_pred))

"""**STOCHASTIC GRADIENT DESCENT**"""

from sklearn.linear_model import SGDClassifier

from sklearn.model_selection import GridSearchCV
grid = {
    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate
    'max_iter': [1000], # number of epochs
    'loss': ['log'], # logistic regression,
    'penalty': ['l2'],
    'n_jobs': [-1]
}
#paramGrid = ParameterGrid(grid)
SGD=SGDClassifier()
grid_obj = GridSearchCV(SGD,grid)
grid_obj = grid_obj.fit(X_train, y_train)
SGD = grid_obj.best_estimator_
SGD.fit(X_train,y_train)
y_pred = SGD.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
print(classification_report(y_test ,y_pred))

"""**PERCEPTRON**"""

from sklearn.linear_model import Perceptron

from sklearn.model_selection import GridSearchCV
grid = {
    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate
    'max_iter': [1000], # number of epochs
    'penalty': ['l2'],
    'n_jobs': [-1]
}
pcpt=Perceptron()
grid_obj = GridSearchCV(pcpt,grid)
grid_obj = grid_obj.fit(X_train, y_train)
pcpt = grid_obj.best_estimator_
pcpt.fit(X_train,y_train)
y_pred = pcpt.predict(X_test)
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
print(classification_report(y_test ,y_pred ))

"""# Random Forest Classifier"""

#Import Random Forest Model
from sklearn.ensemble import RandomForestClassifier

#Create a Gaussian Classifier
clf=RandomForestClassifier()

#Train the model using the training sets y_pred=clf.predict(X_test)
clf.fit(X_train,y_train)

y_pred=clf.predict(X_test)

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [2,4]
# Minimum number of samples required to split a node
min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2]
# Method of selecting samples for training each tree
bootstrap = [True, False]

# Create the param grid
param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
print(param_grid)

from sklearn.model_selection import GridSearchCV
rf_Grid = GridSearchCV(estimator = clf, param_grid = param_grid, cv = 3, verbose=2, n_jobs = 4)

#Train the model using the training sets y_pred=clf.predict(X_test)
rf_Grid.fit(X_train, y_train)

#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
#Model Evaluation on test set

print(classification_report(y_test,y_pred))
print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred))

print('Accuracy: ', accuracy_score(y_test,y_pred))