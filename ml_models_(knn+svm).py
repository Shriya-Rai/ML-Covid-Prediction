# -*- coding: utf-8 -*-
"""ML Models (KNN+SVM).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KE3Gg0l2UH-9VTh6e0T0V9-YQi50KWib
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df=pd.read_csv("/content/drive/MyDrive/Colab Notebooks/corona.csv")

df.info()

data=[df]
genders={'male':0,'female':1,'None':2}
result={'positive':0,"negative":1,"other":2}
age={'Yes':0,'No':1,'None':2}
for dataset in data:
  dataset['gender']=dataset['gender'].map(genders)
  dataset['corona_result']=dataset['corona_result'].map(result)
  dataset['age_60_and_above']=dataset['age_60_and_above'].map(age)

df=df.drop(['test_date', 'test_indication'], axis=1)

data=[df]
for dataset in data:
  dataset['fever']=dataset['fever'].replace("None",2)
  dataset['cough']=dataset['cough'].replace("None",2)
  dataset['shortness_of_breath']=dataset['shortness_of_breath'].replace("None",2)
  dataset['sore_throat']=dataset['sore_throat'].replace("None",2)
  dataset['head_ache']=dataset['head_ache'].replace("None",2)
  dataset['age_60_and_above']=dataset['age_60_and_above'].replace("None",2)
 
df['fever']=df["fever"].astype(int)
df['age_60_and_above']=df["age_60_and_above"].astype(int)
df['head_ache']=df["head_ache"].astype(int)
df['sore_throat']=df["sore_throat"].astype(int)
df['shortness_of_breath']=df["shortness_of_breath"].astype(int)
df['cough']=df["cough"].astype(int)

df

"""**DATA SPLITTING**"""

#Creating a subset of 100000 rows to apply knn and svm
df1=df.iloc[1:100000, : ]
#Split data into 80% training & 20% testing data sets
X = df1.iloc[:, [0, 1, 2, 3, 4, 6, 7]].values
y = df1.iloc[:, [5]].values.ravel()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Creating scaled set to be used in model to improve the results
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""**NAIVE BAYES**"""

from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
y_pred

#Evaluation metrics after prediction of data
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
print(classification_report(y_test ,y_pred ))
print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred))
print()
print('Accuracy: ', accuracy_score(y_test,y_pred))

"""**K-NEAREST NEIGHBORS (HT)**

DATA SUBSET
"""

#Creating a subset of 100000 rows to apply knn and svm
df1=df.iloc[1:100000, : ]
#Split data into 80% training & 20% testing data sets
X = df1.iloc[:, [0, 1, 2, 3, 4, 6, 7]].values
y = df1.iloc[:, [5]].values.ravel()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

# Creating scaled set to be used in model to improve the results
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Import library of KNeighborsClassifier model
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# Create a KNN Classifier
knn = KNeighborsClassifier()

# Hyperparameter Optimization
parameters = {'n_neighbors': [3, 5, 10], 
              'weights': ['uniform', 'distance'],
              'algorithm' : ['auto', 'brute'],
              'leaf_size' : [10, 20]
             }

# Run the grid search
grid_obj = GridSearchCV(knn, parameters)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the knn to the best combination of parameters
knn = grid_obj.best_estimator_

# Train the model using the training sets 
knn.fit(X_train,y_train)

# Prediction on test data
y_pred = knn.predict(X_test)

#Evaluation metrics after prediction of data
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
print(classification_report(y_test ,y_pred ))
print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred))
print()
print('Accuracy: ', accuracy_score(y_test,y_pred))

"""**SUPPORT VECTOR MACHINE (HT)**

DATA SUBSET
"""

#Creating a subset of 50000 rows to apply knn and svm
df1=df.iloc[1:100000, : ]
#Split data into 80% training & 20% testing data sets
X = df1.iloc[:, [0, 1, 2, 3, 4, 6, 7]].values
y = df1.iloc[:, [5]].values.ravel()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

from sklearn import svm

# Create a Support Vector Classifier
svc = svm.SVC()

# Hyperparameter Optimization
parameters = [
  {'C': [1, 100], 'kernel': ['linear']},
  {'C': [1, 100], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]

# Run the grid search
grid_obj = GridSearchCV(svc, parameters)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the svc to the best combination of parameters
svc = grid_obj.best_estimator_

# Train the model using the training sets 
svc.fit(X_train,y_train)

# Prediction on test data
y_pred = svc.predict(X_test)

#Evaluation metrics after prediction of data
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
print(classification_report(y_test ,y_pred ))
print('Confusion Matrix: \n', confusion_matrix(y_test,y_pred))
print()
print('Accuracy: ', accuracy_score(y_test,y_pred))